calendar_id: chinese-model-releases
title: Chinese AI Model Releases
description: Public release announcements for Chinese AI models including Baidu, Alibaba, Tencent, and others.
locale: zh-CN
timezone: Asia/Shanghai
maintainers:
  - data-team
tags:
  - ai
  - chinese
  - model-release
update_frequency: on-demand
events:
  - id: baidu-wenxin-yiyan-2023-03-16
    title: Baidu Wenxin Yiyan (Ernie Bot) announced
    date: 2023-03-16
    all_day: true
    source: https://ernie.baidu.com/
    notes: Baidu's large language model chatbot, full name "Enhanced Representation
      through Knowledge Integration". Built on the ERNIE series of LLMs which have
      been in development since 2019. Initially launched for invited testing on this
      date, with public release following on August 31, 2023 after receiving regulatory
      approval. Reached over 200 million users by April 2024.
    tags:
      - baidu
      - wenxin-yiyan
      - ernie-bot
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
  - id: alibaba-tongyi-qianwen-2023-04-11
    title: Alibaba Tongyi Qianwen announced
    date: 2023-04-11
    all_day: true
    source: https://www.reuters.com/technology/alibaba-unveils-tongyi-qianwen-an-ai-model-similar-gpt-2023-04-11/
    notes: Alibaba Cloud unveiled Tongyi Qianwen (Qwen), a large language model developed
      to compete with GPT. The Chinese name literally means "to comprehend the meaning,
      and to answer a thousand kinds of questions." Initially launched for beta testing
      in April 2023, with public release following in September 2023 after receiving
      regulatory approval. Many Qwen variants are released as open-weight models under
      the Apache-2.0 license, with models ranging from 1.8B to 235B parameters.
    tags:
      - alibaba
      - tongyi-qianwen
      - qwen
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
  - id: zhipu-chatglm-6b-2023-03-16
    title: Zhipu AI ChatGLM-6B released
    date: 2023-03-16
    all_day: true
    source: https://github.com/THUDM/ChatGLM-6B
    notes: Zhipu AI and Tsinghua KEG Lab released ChatGLM-6B, an open-source bilingual
      (Chinese-English) dialogue language model with 6.2 billion parameters. Based on
      the General Language Model (GLM) architecture, it was trained on approximately 1T
      tokens of Chinese and English corpus. The model supports local deployment on
      consumer-grade GPUs (6GB VRAM with INT4 quantization) and became one of the most
      popular open-source Chinese LLMs.
    tags:
      - zhipu
      - chatglm
      - chatglm-6b
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
  - id: zhipu-chatglm2-6b-2023-06-25
    title: Zhipu AI ChatGLM2-6B released
    date: 2023-06-25
    all_day: true
    source: https://github.com/THUDM/ChatGLM2-6B
    notes: Zhipu AI released ChatGLM2-6B, the second-generation version of ChatGLM-6B.
      Key improvements include 1.4T tokens of pretraining data (vs 1T), context length
      extended from 2K to 32K tokens using FlashAttention, 42% faster inference speed,
      and significant performance gains on benchmarks (MMLU +23%, C-Eval +33%,
      GSM8K +571%). The model supports multi-query attention and maintains the ability
      to run on consumer-grade hardware.
    tags:
      - zhipu
      - chatglm
      - chatglm2
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
  - id: zhipu-chatglm3-2023-10-27
    title: Zhipu AI ChatGLM3 released
    date: 2023-10-27
    all_day: true
    source: https://github.com/THUDM/ChatGLM3
    notes: Zhipu AI announced ChatGLM3 at the Zhipu AI Dev Day. The third generation
      includes ChatGLM3-6B with stronger base model capabilities (best performance
      among sub-10B models), native support for function calling, code interpreter,
      and agent tasks. The release also included base model (ChatGLM3-6B-Base) and
      extended context variants (32K and 128K). This marked Zhipu's continued
      commitment to open-source Chinese LLMs.
    tags:
      - zhipu
      - chatglm
      - chatglm3
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
  - id: moonshot-kimi-chat-2023-11-16
    title: Moonshot AI Kimi Chat public release
    date: 2023-11-16
    all_day: true
    source: https://en.wikipedia.org/wiki/Kimi_(chatbot)
    notes: Moonshot AI (月之暗面) released Kimi Chat to the general public based on their
      "Moonshot" large language model. Moonshot AI was founded in March 2023, released
      Kimi for closed beta testing in October 2023, and opened it to the public on
      November 16, 2023. The first version supported 128,000 tokens of context
      (approximately 200,000 Chinese characters), making it the first AI model capable
      of accepting contexts of this size. Later versions expanded context to 2 million
      characters. The model is available via web app, iOS, and Android.
    tags:
      - moonshot
      - moonshot-ai
      - kimi
      - kimi-chat
      - 月之暗面
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z

  - id: deepseek-llm-2024-01-05
    title: DeepSeek LLM (Chat) released
    date: 2024-01-05
    all_day: true
    source: https://github.com/deepseek-ai/DeepSeek-LLM
    notes: DeepSeek released their first-generation large language model series including
      7B and 67B parameter versions (both base and chat). Trained from scratch on 2 trillion
      tokens in English and Chinese. The 67B Chat model outperformed Llama2-70B and GPT-3.5
      in various benchmarks, particularly in coding (HumanEval 73.78%), mathematics
      (GSM8K 84.1%), and Chinese comprehension. The model supports commercial use and
      intermediate checkpoints were also released.
    tags:
      - deepseek
      - deepseek-ai
      - deepseek-llm
      - model-release
      - llm
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z

  - id: deepseek-coder-2024-01-25
    title: DeepSeek Coder released
    date: 2024-01-25
    all_day: true
    source: https://github.com/deepseek-ai/DeepSeek-Coder
    notes: DeepSeek released a series of code language models ranging from 1.3B to 33B
      parameters, trained from scratch on 2 trillion tokens (87% code, 13% natural language).
      Features project-level code completion with 16K window size and fill-in-the-blank
      task support. Achieved state-of-the-art performance among open-source code models
      on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks. DeepSeek-Coder-Instruct-33B
      outperformed GPT-3.5-turbo on coding tasks.
    tags:
      - deepseek
      - deepseek-ai
      - deepseek-coder
      - model-release
      - llm
      - code-model
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z

  - id: deepseek-v2-2024-05-06
    title: DeepSeek-V2 released
    date: 2024-05-06
    all_day: true
    source: https://github.com/deepseek-ai/DeepSeek-V2
    notes: DeepSeek released V2, a Mixture-of-Experts (MoE) language model with 236B total
      parameters and 21B activated per token. Introduced innovative Multi-head Latent
      Attention (MLA) and DeepSeekMoE architectures for efficient training and inference.
      Compared to DeepSeek 67B, V2 saved 42.5% training costs, reduced KV cache by 93.3%,
      and boosted throughput 5.76x. Supports 128K context length. A lighter version,
      DeepSeek-V2-Lite (16B total, 2.4B activated), was released on May 16, 2024.
    tags:
      - deepseek
      - deepseek-ai
      - deepseek-v2
      - model-release
      - llm
      - moe
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z

  - id: deepseek-v3-2024-12-27
    title: DeepSeek-V3 released
    date: 2024-12-27
    all_day: true
    source: https://github.com/deepseek-ai/DeepSeek-V3
    notes: DeepSeek released V3, a 671B parameter MoE model with 37B activated per token.
      Features FP8 mixed precision training framework, auxiliary-loss-free load balancing,
      and Multi-Token Prediction (MTP) objective. Trained on 14.8 trillion tokens with
      only 2.788M H800 GPU hours. The model achieves performance comparable to leading
      closed-source models like GPT-4o and Claude-3.5-Sonnet, particularly excelling
      in math (MATH-500 90.2%) and code (HumanEval 65.2%) tasks.
    tags:
      - deepseek
      - deepseek-ai
      - deepseek-v3
      - model-release
      - llm
      - moe
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z

  - id: deepseek-r1-2025-01-22
    title: DeepSeek-R1 released
    date: 2025-01-22
    all_day: true
    source: https://github.com/deepseek-ai/DeepSeek-R1
    notes: DeepSeek released R1, their first-generation reasoning model trained via
      large-scale reinforcement learning (RL) without supervised fine-tuning as a
      preliminary step. R1-Zero demonstrated that reasoning capabilities can emerge
      purely through RL. R1 incorporates cold-start data before RL and achieves
      performance comparable to OpenAI-o1 across math (AIME 2024 79.8%), code, and
      reasoning tasks. Also released distilled models (1.5B to 70B) based on Qwen
      and Llama that outperform OpenAI-o1-mini.
    tags:
      - deepseek
      - deepseek-ai
      - deepseek-r1
      - model-release
      - llm
      - reasoning
    status: confirmed
    updated_at: 2026-02-19T00:00:00Z
